% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MSmix_functions_package.R
\name{rMSmoe}
\alias{rMSmoe}
\title{Random samples from a MoE of Mallows models with Spearman distance}
\usage{
rMSmoe(
  n_items,
  n_clust = 2,
  X,
  rho = NULL,
  theta = NULL,
  betas = NULL,
  uniform = FALSE,
  mh = TRUE
)
}
\arguments{
\item{n_items}{Number of items.}

\item{n_clust}{Number of mixture components. Defaults to 2.}

\item{X}{Numeric \eqn{N}\eqn{\times}{x}\eqn{H+1} design matrix.}

\item{rho}{Integer \eqn{G}\eqn{\times}{x}\eqn{n} matrix with the component-specific consensus rankings in each row. Defaults to \code{NULL}, meaning that the consensus rankings are randomly generated according to the sampling scheme indicated by the \code{uniform} argument. See Details.}

\item{theta}{Numeric vector of \eqn{G} non-negative component-specific precision parameters. Defaults to \code{NULL}, meaning that the concentrations are uniformly generated from an interval containing typical values for the precisions. See Details.}

\item{betas}{Numeric \eqn{G}\eqn{\times}{x}\eqn{H+1} matrix of coefficients of the GLM.}

\item{uniform}{Logical: whether \code{rho} or \code{weights} have to be sampled uniformly on their support. When \code{uniform = FALSE} they are sampled, respectively, to ensure separation among mixture components and populated weights. Used when \eqn{G>1} and either \code{rho} or \code{weights} are \code{NULL} (see Details). Defaults to \code{FALSE}.}

\item{mh}{Logical: whether the samples must be drawn with the Metropolis-Hastings (MH) scheme implemented in the \code{BayesMallows} package, rather by direct sampling from the Mallows probability distribution. For \code{n_items > 10}, the MH is always applied to speed up the sampling procedure. Defaults to \code{TRUE}.}
}
\value{
A list of the following named components:

\item{\code{samples}}{Integer \eqn{N}\eqn{\times}{x}\eqn{n} matrix with the \code{sample_size} simulated full rankings in each row.}
\item{\code{rho}}{Integer \eqn{G}\eqn{\times}{x}\eqn{n} matrix with the component-specific consensus rankings used for the simulation in each row.}
\item{\code{theta}}{Numeric vector of the \eqn{G} component-specific precision parameters used for the simulation.}
\item{\code{betas}}{Numeric \eqn{G}\eqn{\times}{x}\eqn{H+1} matrix of coefficients of the GLM used for the simulation.}
\item{\code{weights}}{Numeric \eqn{N}\eqn{\times}{x}\eqn{G} matrix of the covariate-dependent class membership probabilities.}
\item{\code{classification}}{Integer vector of the \code{sample_size} component membership labels.}
}
\description{
Draw random samples of full rankings from a mixture of experts of Mallows models with Spearman distance.
}
\details{
The case with a single mixture component (\eqn{G=1}) implies that covariates are not allowed and coincides with the homogeneous Mallows models with Spearman distance. The case with multiple mixture components (\eqn{G>1}) and absence of covariates coincides with the mixture of MMS, thus the functions is equivalent to \code{rMSmix}.

When \code{n_items > 10} or \code{mh = TRUE}, the random samples are obtained by using the Metropolis-Hastings algorithm, described in Vitelli et al. (2018) and implemented in the \code{sample_mallows} function of the package \code{BayesMallows} package.

When \code{theta = NULL} is not provided by the user, the concentration parameters are randomly generated from a uniform distribution on the interval \eqn{(1/n^{2},3/n^{1.5})} of some typical values for the precisions.

When \code{uniform = FALSE}, the mixing weights are sampled from a symmetric Dirichlet distribution with shape parameters all equal to \eqn{2G}, to favor populated and balanced clusters;
the consensus parameters are sampled to favor well-separated clusters, i. e.,  at least at Spearman distance \eqn{\frac{2}{G}\binom{n+1}{3}} from each other.
}
\examples{

## Example 1.

## Example 2.

## Example 3.

}
\references{
Crispino M, Modugno L and Mollica C (2025). Integrating covariates in mixtures of Mallows models with Spearman distance for analysing preference rankings.

Vitelli V, Sørensen Ø, Crispino M, Frigessi A and Arjas E (2018). Probabilistic Preference Learning with the Mallows Rank Model. \emph{Journal of Machine Learning Research}, \bold{18}(158), pages 1--49, ISSN: 1532-4435, \href{https://jmlr.org/papers/v18/15-481.html}{https://jmlr.org/papers/v18/15-481.html}.

Sørensen Ø, Crispino M, Liu Q and Vitelli V (2020). BayesMallows: An R Package for the Bayesian Mallows Model. \emph{The R Journal}, \bold{12}(1), pages 324--342, DOI: 10.32614/RJ-2020-026.

Chenyang Zhong (2021). Mallows permutation model with L1 and L2 distances I: hit and run algorithms and mixing times. arXiv: 2112.13456.
}
